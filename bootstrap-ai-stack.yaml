---
- name: Install Ollama + Open WebUI with full NVIDIA GPU support on Ubuntu 24.04
  hosts: localhost
  become: true
  connection: local

  tasks:
    - name: Remove any old/broken repo files
      ansible.builtin.file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/apt/sources.list.d/docker.list
        - /etc/apt/sources.list.d/docker.sources
        - /etc/apt/sources.list.d/nvidia-container-toolkit.list

    - name: Add Docker GPG key
      ansible.builtin.get_url:
        url: https://download.docker.com/linux/ubuntu/gpg
        dest: /etc/apt/keyrings/docker.asc
        mode: '0644'

    - name: Add Docker repository (deb822 format)
      ansible.builtin.copy:
        dest: /etc/apt/sources.list.d/docker.sources
        content: |
          Types: deb
          URIs: https://download.docker.com/linux/ubuntu
          Suites: noble
          Components: stable
          Signed-By: /etc/apt/keyrings/docker.asc
        mode: '0644'

    - name: Add NVIDIA Container Toolkit repository
      ansible.builtin.shell: |
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
        curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list |
          sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' |
          tee /etc/apt/sources.list.d/nvidia-container-toolkit.list > /dev/null
      args:
        creates: 
          - /etc/apt/sources.list.d/nvidia-container-toolkit.list
          - /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

    - name: Update apt cache
      ansible.builtin.apt:
        update_cache: true

    - name: Install required packages
      ansible.builtin.apt:
        name: 
          - python3-docker
          - python3-compose
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-compose-plugin
          - nvidia-container-toolkit
        state: present

    - name: Configure Docker for NVIDIA runtime
      ansible.builtin.command: nvidia-ctk runtime configure --runtime=docker
      args:
        creates: /etc/docker/daemon.json

    - name: Restart Docker
      ansible.builtin.systemd:
        name: docker
        state: restarted

    - name: Create directories
      ansible.builtin.file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /opt/ai-stack
        - /opt/ai-stack/ollama
        - /opt/ai-stack/open-webui

    - name: Deploy docker-compose.yml
      ansible.builtin.copy:
        dest: /opt/ai-stack/docker-compose.yml
        mode: '0644'
        content: |
          services:
            ollama:
              image: ollama/ollama:latest
              container_name: ollama
              restart: unless-stopped
              ports:
                - 11434:11434
              volumes:
                - /opt/ai-stack/ollama:/root/.ollama
              environment:
                - OLLAMA_HOST=0.0.0.0
              runtime: nvidia

            open-webui:
              image: ghcr.io/open-webui/open-webui:main
              container_name: open-webui
              restart: unless-stopped
              ports:
                - 3000:8080
              volumes:
                - /opt/ai-stack/open-webui:/app/backend/data
              depends_on:
                - ollama
              environment:
                - OLLAMA_BASE_URL=http://ollama:11434
                - WEBUI_AUTH=False

    - name: Start services
      community.docker.docker_compose:
        project_src: /opt/ai-stack
        state: present

    - name: Wait for Ollama
      ansible.builtin.uri:
        url: http://localhost:11434
        method: GET
      register: result
      until: result.status == 200
      retries: 40
      delay: 5

    - name: Pull starter models if missing
      ansible.builtin.shell: |
        docker exec ollama ollama list | grep -qw {{ item }} || docker exec ollama ollama pull {{ item }}
      loop:
        - llama3.2:latest
        - gemma3:12b
        - phi3:14b
        - nomic-embed-text:latest
      ignore_errors: true
